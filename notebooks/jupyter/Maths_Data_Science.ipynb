{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Maths in machine learning**\n",
    "\n",
    "### 1. Algebra of vectors and matrices\n",
    "\n",
    "#### 1.1 Vectors\n",
    "\n",
    "We have this vector representation:\n",
    "$$ x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} $$\n",
    "\n",
    "The transpose of the two-component column vector is:\n",
    "$$ x^\\mathsf{T}= \\begin{pmatrix} x_1 , x_2 \\end{pmatrix} $$\n",
    "\n",
    "The sum of two column vectors is given by:\n",
    "$$ x+y = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} + \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} x_1+y_1 \\\\ x_2+y_2 \\end{pmatrix} $$\n",
    "\n",
    "And the inner product by:\n",
    "$$ x^\\mathsf{T}y = \\begin{pmatrix} x_1 , x_2 \\end{pmatrix}  \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = x_1y_1 + x_2y_2 $$\n",
    "\n",
    "The length or euclidean norm of the vector $x$ is:\n",
    "$$ \\Vert x \\Vert = \\sqrt{x_1^2 + x_2^2} = \\sqrt{x^\\mathsf{T}x}$$\n",
    "\n",
    "As we can see, the inner product of $x$ and $y$ can be expressed in terms of the vector lenghts and the angle $\\theta$ between the two vectors:\n",
    "$$ x^\\mathsf{T}y = \\Vert x \\Vert \\Vert y \\Vert \\cos\\theta$$\n",
    "\n",
    "If $\\theta$ is 90 degrees, then the vectors are said to be *orthogonal*, in which case: \n",
    "$$ x^\\mathsf{T}y = 0$$\n",
    "\n",
    "We can see that any vector can be expressed in terms of orthogonal *unit vectors*:\n",
    "$$ x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x_1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + x_2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} $$\n",
    "\n",
    "$$ x = x_1i + x_2j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0. Load libraries and custom modules\n",
    "# Data -----------------------------------------------------------------\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.int64(-1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define two vectors\n",
    "a = np.array([1,2])\n",
    "b = np.array([1,-1])\n",
    "# Addition\n",
    "display(np.add(a, b))\n",
    "# Subtraction\n",
    "display(np.subtract(a, b))\n",
    "# Inner product\n",
    "display(np.inner(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Matrices\n",
    "\n",
    "A 2x2 matrix is written in the form\n",
    "\n",
    "$$ \\mathbf{A} = \\begin{pmatrix} a_{11} & a_{12}\\\\ a_{21} & a_{22} \\end{pmatrix} $$\n",
    "\n",
    "The notation per each element in the matrix is: first index means row, second index means column. When a matrix is multiplied with a vector, the result is another vector:\n",
    "\n",
    "$$ \\mathbf{A}x = \\begin{pmatrix} a_{11} & a_{12}\\\\ a_{21} & a_{22} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} a_{11}x_1 + a_{12}x_2 \\\\ a_{21}x_1 + a_{22}x_2 \\end{pmatrix} $$\n",
    "\n",
    "In general, for $ \\mathbf{A} = (a_1, a_2 ... a_N)$, where the vectors $a_i$ are the columns of $\\mathbf{A}$:\n",
    "$$ \\mathbf{A}x = x_1a_1 + x_2a_2 + ... + x_Na_N $$\n",
    "\n",
    "The product of two 2x2 matrices is given by:\n",
    "$$ \\mathbf{A}\\mathbf{B} = \\begin{pmatrix} a_{11} & a_{12}\\\\ a_{21} & a_{22} \\end{pmatrix} \\begin{pmatrix} b_{11} & b_{12}\\\\ b_{21} & b_{22} \\end{pmatrix} = \\begin{pmatrix} a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\ a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\end{pmatrix}$$\n",
    "\n",
    "The matrix product is allowed whenever $\\mathbf{A}$ has the same number of columns as $\\mathbf{B}$ has rows. So for this case, if $\\mathbf{A}$ has dimension $l$ x $m$ and $\\mathbf{B}$ has dimension $m$ x $n$ then $\\mathbf{A}\\mathbf{B}$ is $l$ x $n$ with elements:\n",
    "$$ (\\mathbf{A}\\mathbf{B})_{ij} = (\\sum_{k=1}^m a_{ik}b_{kj}) : i= 1...l, j= 1...n$$\n",
    "\n",
    "Note that matrix multiplication is not commutative, this means $\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}$ in general. However it is associative:\n",
    "$$ (\\mathbf{A}\\mathbf{B})\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})$$\n",
    "\n",
    "Matrices, like vectors have a transposed form, which is obtained by mirroring the values along the diagonal:\n",
    "$$ \\mathbf{A} =  \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix} $$\n",
    "$$ \\mathbf{A}^\\mathsf{T} = \\begin{pmatrix} a_{11} & a_{21} \\\\ a_{12} & a_{22} \\end{pmatrix}$$\n",
    "\n",
    "Transposition has the properties:\n",
    "\n",
    "$$ (\\mathbf{A} + \\mathbf{B})^\\mathsf{T} = \\mathbf{A}^\\mathsf{T} + \\mathbf{B}^\\mathsf{T}$$\n",
    "$$ (\\mathbf{A}\\mathbf{B})^\\mathsf{T} = \\mathbf{B}^\\mathsf{T}\\mathbf{A}^\\mathsf{T}$$\n",
    "\n",
    "We have an special matrix, the square matrix which has equal number of rows and columns. The determinant of a $p$ x $p$ square matrix is written $|\\mathbf{A}|$ and is defined as:\n",
    "$$ |\\mathbf{A}|= \\sum_{(j_1,..,j_p)} (-1)^{f(j1..jp)}a_{1j_1}a_{2j_2}...a_{pj_p}$$\n",
    "\n",
    "For example the determinant of a 2x2 matrix is given by\n",
    "\n",
    "$$ |\\mathbf{A}| = a_{11}a_{22} - a_{12}a_{21}$$\n",
    "\n",
    "Some of the determinant properties are:\n",
    "\n",
    "$$ |\\mathbf{A}\\mathbf{B}| = |\\mathbf{A}| |\\mathbf{B}|$$\n",
    "$$ |\\mathbf{A}^\\mathsf{T}| = |\\mathbf{A}| $$\n",
    "\n",
    "Another special matrix is the identity matrix, which is square with ones along its diagonal and zeroes everywhere. For example:\n",
    "\n",
    "$$ \\mathbf{I} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$$\n",
    "\n",
    "And for any $\\mathbf{A}$, we have:\n",
    "\n",
    "$$ \\mathbf{I}\\mathbf{A} = \\mathbf{A}\\mathbf{I} = \\mathbf{A}$$\n",
    "\n",
    "The matrix inverse $\\bm{A}^{-1}$ of a square matrix $\\bm{A}$ is defined in terms of the identity matrix by the requirements:\n",
    "\n",
    "$$ \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}$$\n",
    "\n",
    "In the case of a 2x2 matrix the inverse is:\n",
    "\n",
    "$$ \\mathbf{A}^{-1} = \\frac{1}{|\\mathbf{A}|}\\begin{pmatrix} a_{22} & -a_{12} \\\\ -a_{21} & a_{11} \\end{pmatrix}$$\n",
    "\n",
    "The matrix inverse has the following properties:\n",
    "\n",
    "$$ (\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}$$\n",
    "$$ (\\mathbf{A}^{-1})^{\\mathsf{T}} = (\\mathbf{A}^{\\mathsf{T}})^{-1}$$\n",
    "\n",
    "An orthonormal maxtrix has its inverse as its transpose:\n",
    "\n",
    "$$ \\mathbf{A}^{\\mathsf{T}}\\mathbf{A} = \\mathbf{I}$$\n",
    "\n",
    "The trace of a square matrix is the sum of its diagonal elements. For example for a 2x2 matrix:\n",
    "\n",
    "$$tr(\\mathbf{A}) = a_{11}+a_{22}$$\n",
    "\n",
    "Some of the properties of the trace are:\n",
    "\n",
    "$$ tr(\\mathbf{A}+\\mathbf{B}) = tr\\mathbf{A} + tr\\mathbf{B}$$\n",
    "$$ tr(\\mathbf{A}\\mathbf{B}) = tr(\\mathbf{B}\\mathbf{A})$$\n",
    "\n",
    "A singular matrix has its determinant equal to 0, then $\\mathbf{A}$ has no inverse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 System of equations\n",
    "\n",
    "A system of n linear equations of the form:\n",
    "\n",
    "$$ y_i = \\sum_{j=1}^{n}a_jx_j(i), i=1..n$$\n",
    "\n",
    "can be written in matrix notation as:\n",
    "\n",
    "$$ \\bm{y} = \\bm{A}a $$\n",
    "\n",
    "where $\\bm{y} = (y_1..u_n)^{\\mathsf{T}}$, $ \\bm{a}=(a_1..a_n)^{\\mathsf{T}}$ y $\\bm{A}_{ij} = x_j(i)$. If $\\bm{A}$ is nonsingular, the solution for the parameter vector $\\bm{a}$ is given by:\n",
    "\n",
    "$$ \\bm{a} = \\bm{A}^{-1}\\bm{y}$$\n",
    "\n",
    "If $ \\bm{A} $ is nonsingular, then the equation \n",
    "\n",
    "$$\\bm{A}\\bm{x} = \\bm{0} $$\n",
    "\n",
    "only has a trivial solution  $\\bm{x}=0$\n",
    "\n",
    "The Gaussian elimination is a technique for solving a system of equations, where there are 3 possible solutions:\n",
    "- There is a solution\n",
    "- System doesn't have a solution\n",
    "- System has infinite number of solutions\n",
    "\n",
    "The process is to convert the system to matrix vector equation, to convert the matrix to ones on diagonals and mapping the matrix back to equation. For example, given this equation:\n",
    "\n",
    "$$ 2x_1 + x_2 = 4$$\n",
    "$$ 3x_1 - 2x_2 = -1 $$\n",
    "\n",
    "We convert this into a matrix vector\n",
    "\n",
    "$$  \\begin{pmatrix} 2 & 1 \\\\ 3 & -2 \\end{pmatrix} \\begin{pmatrix}4 \\\\ -1 \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Eigenvalues and eigenvectors\n",
    "\n",
    "The eigen problem consist of finding eigenvectors ${u}$ and eigenvalues $\\lambda$ that satisfy the matrix equation:\n",
    "$$ {A}{u} = {\\lambda}{u}$$\n",
    "where ${A}$ is both symmetric and positive definite. Geometrically, we seek special vectors ${u}$ that, when matrix multiplied with ${A}$, change at most their sign an length but not their direction: these are the \"own\" or \"eigen\" vectos of ${A}$. So we can write the following equation:\n",
    "$$ ({A}-{\\lambda}{I}){u}={0}$$\n",
    "so for a nontrivial solution for ${u}$ we must have that the determinant is zero:\n",
    "$$ |{A}-{\\lambda}{I}|=0$$\n",
    "This is known as the characteristic equation for the matrix ${A}$. In the case of a 2x2 matrix, we have\n",
    "$$ \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix} \\begin{pmatrix} u_{1} \\\\ u_{2} \\end{pmatrix} = \\lambda \\begin{pmatrix} u_{1} \\\\ u_{2} \\end{pmatrix}$$\n",
    "And the determinant would be applied to the matrix\n",
    "$$ \\begin{pmatrix} a_{11} - \\lambda & a_{12} \\\\ a_{21} & a_{22} - \\lambda \\end{pmatrix} $$\n",
    "\n",
    "In order to find the eigenvalues, we'll use a trick from 3Blue1Brown. They mention that there are some properties to remember\n",
    "1. $$  tr({A}) = \\lambda_1 + \\lambda_2$$\n",
    "2. $$  det\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = ad - bc = \\lambda_1 \\lambda_2$$\n",
    "For 1, we can halve the results so we obtain a mean $m$ equal to $tr({A})/2$. And $p$ is the result of $\\lambda 1 \\lambda 2$ So we can derive the third equation:\n",
    "\n",
    "3. $$ \\lambda_1, \\lambda_2 = m \\pm \\sqrt{m^2-p} $$\n",
    "\n",
    "Now we can obtain the eigenvectors by substituting $\\lambda$ into the general equation. One of the properties of the eigenvectors is that they are orthogonal. For our case:\n",
    "$$ (u_1)^{\\mathsf{T}}(u_2) = 0$$\n",
    "\n",
    "Moreover, we can multiply our eigenproblem equation by any factor, so we can choose to eigenvector of unit length $||u|| = 1 $. The matrix formed by such eigenvectors:\n",
    "$$ {U} = (u_1, u_2, ... u_n) = \\begin{pmatrix} u_1 & u_2 & .. \\\\ u_1 & u_2 & .. \\\\ .. & .. & .. \\end{pmatrix}$$\n",
    "its said to diagonalize the matrix ${A}$. That is, if ${A}$ is multiplied from the left by ${U^{\\mathsf{T}}}$ and from the right by ${U}$, the result is a diagonal matrix with the eigenvalue along the diagonal:\n",
    "$$ {U^{\\mathsf{T}}} {A} {U} = \\varLambda = \\begin{pmatrix} \\lambda_1 & 0 & .. \\\\ 0 & \\lambda_2 & .. \\\\ .. & .. & .. \\end{pmatrix}$$\n",
    "Note that ${U}$ is an orthonormal matrix: ${U^{\\mathsf{T}}}{U}={I}$\n",
    "And we ca say that ${A}$ is positive definite if and only if its eigenvalues $\\lambda_i, i=1...N$ are positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Singular value decomposition\n",
    "We can factorize then any matrix ${A}$ into a product of an orthonormal matrix ${U}$ times a diagonal matrix ${\\varLambda}$ whose diagonals are the eigenvalues of ${A}$ times the transpose of ${U}$.\n",
    "$$ {A} = {U}{\\varLambda}{U^{\\mathsf{T}}}  $$\n",
    "This is also called the eigendecomposition or spectral decomposition of ${A}$\n",
    "\n",
    "SVD is a powerful tool for the solution of linear equations and is often used when a solution cannot be determined by other numercial algorithms. For example, to invert a non-singular symmetric matrix ${A}$, we simply write\n",
    "$$ {A^{-1}} = {U}{\\varLambda^{-1}}{U^{\\mathsf{T}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Solve Linear Regression with Linear Algebra\n",
    "\n",
    "Linear regression is a method for modeling the relationship between two scalar values: the input variable x and the output variable y. The model assumes that y is a linear function of the input variable $y=f(x)$. For a simple case:\n",
    "$$ y = \\beta_0 + \\beta_1x_1$$\n",
    "\n",
    "So we can state linear regression using matrix notation:\n",
    "$$ y = {X}b $$\n",
    "\n",
    "Since in data we find same combinations that give different results, we will have some error due to not having an exact solution. So we need to find a solution where we minimize the error:\n",
    "$$ e(\\beta) = y - {X}b $$\n",
    "We can use the MSE (mean squared error):\n",
    "$$ MSE(\\beta) = \\frac{1}{n}\\sum_{i=1}^{n}e_i^{2}(\\beta)$$\n",
    "\n",
    "After some derivations not shown on this class, we'll have:\n",
    "$$ \\beta = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Convolution\n",
    "\n",
    "It is defined as the integral of the product of the two functions after one is reflected about the y-axis and shifted\n",
    "$$(f * g)(t) := \\int_{-\\infty}^\\infty f(\\tau) g(t - \\tau) \\, d\\tau$$\n",
    "\n",
    "For discrete values we have:\n",
    "$$G= f*g := G[i,j] = \\sum_{u=k}^{k}\\sum_{v=-k}^{k}f[u,v]g[i+u,j+v]$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
